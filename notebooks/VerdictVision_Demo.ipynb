{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç VerdictVision: AI-Powered Legal Analytics\n",
    "\n",
    "**CMPE 258 - Deep Learning Project**\n",
    "\n",
    "A RAG-based system for analyzing California appellate cases using hybrid retrieval and Microsoft Phi-2 LLM.\n",
    "\n",
    "## Features\n",
    "- üìö **Hybrid Retrieval**: Semantic + TF-IDF + Metadata scoring\n",
    "- ü§ñ **LLM-Powered Q&A**: Answer legal questions with case citations\n",
    "- ‚öñÔ∏è **IRAC Analysis**: Generate structured legal analysis\n",
    "- üìä **Outcome Prediction**: Predict case outcomes based on similar cases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if running from GitHub)\n",
    "# !git clone https://github.com/YOUR_USERNAME/VerdictVision.git\n",
    "# %cd VerdictVision\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch transformers sentence-transformers pandas numpy scikit-learn\n",
    "!pip install -q gensim requests beautifulsoup4 gdown matplotlib seaborn gradio tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from src.data_collection import DataCollector\n",
    "from src.preprocessing import CasePreprocessor\n",
    "from src.embeddings import EmbeddingManager\n",
    "from src.retrieval import HybridRetriever\n",
    "from src.qa_system import VerdictVisionQA\n",
    "from src.evaluation import OutcomePredictionEvaluator, RetrievalEvaluator\n",
    "from configs.config import ensure_directories\n",
    "\n",
    "ensure_directories()\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Download California appellate case data from Case.law API or Google Drive backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Google Drive (faster)\n",
    "collector = DataCollector()\n",
    "collector.download_from_gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline\n",
    "\n",
    "Extract text, clean documents, and create chunks for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "preprocessor = CasePreprocessor()\n",
    "df, chunks = preprocessor.run_pipeline()\n",
    "\n",
    "print(f\"\\n‚úì Processed {len(df)} cases into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Retrieval System\n",
    "\n",
    "Create embeddings and TF-IDF index for hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from src.embeddings import create_embeddings_for_chunks\n",
    "\n",
    "embeddings = create_embeddings_for_chunks()\n",
    "print(f\"\\n‚úì Created embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "retriever = HybridRetriever()\n",
    "retriever.load_data()\n",
    "\n",
    "# Example search\n",
    "results = retriever.hybrid_search(\"breach of contract damages\", top_k=3)\n",
    "\n",
    "print(\"\\nüîç Search Results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] {r['case_name']}\")\n",
    "    print(f\"    Score: {r['scores']['final']:.4f}\")\n",
    "    print(f\"    Preview: {r['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Q&A System\n",
    "\n",
    "Load Phi-2 LLM and create the complete Q&A system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the complete system\n",
    "qa_system = VerdictVisionQA()\n",
    "qa_system.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo: Legal Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a legal question\n",
    "question = \"What are the elements of breach of contract in California?\"\n",
    "\n",
    "result = qa_system.query(question, mode=\"qa\")\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"\\nüí° Answer:\\n{result['answer']}\")\n",
    "print(f\"\\nüìö Sources:\")\n",
    "for i, case in enumerate(result['cases'][:3], 1):\n",
    "    print(f\"   {i}. {case['case_name']}\")\n",
    "print(f\"\\n‚è±Ô∏è Latency: {result['latency_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demo: Outcome Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict case outcome\n",
    "case_text = \"\"\"\n",
    "Carolina PONCIO, Plaintiff and Appellant, v. DEPARTMENT OF RESOURCES \n",
    "RECYCLING AND RECOVERY, Defendant and Respondent. The plaintiff held \n",
    "a probationary certificate to operate a beverage container recycling center.\n",
    "The department revoked the certificate after finding that the plaintiff's \n",
    "employee engaged in dishonesty by offering a bribe.\n",
    "\"\"\"\n",
    "\n",
    "result = qa_system.query(case_text, mode=\"predict\")\n",
    "\n",
    "print(f\"‚öñÔ∏è Predicted Outcome: {result['predicted_outcome'].upper()}\")\n",
    "print(f\"üìä Confidence: {result['confidence']*100:.1f}%\")\n",
    "print(f\"\\nüìö Similar Cases:\")\n",
    "for i, case in enumerate(result['similar_cases'][:3], 1):\n",
    "    print(f\"   {i}. {case['case_name']} ({case.get('outcome', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run outcome prediction evaluation\n",
    "evaluator = OutcomePredictionEvaluator()\n",
    "evaluator.load_data()\n",
    "\n",
    "# Train baseline\n",
    "baseline_acc, report = evaluator.train_baseline()\n",
    "\n",
    "# Majority baseline\n",
    "majority_acc = evaluator.compute_majority_baseline()\n",
    "\n",
    "print(f\"\\nüìä Results Summary:\")\n",
    "print(f\"   Majority Baseline: {majority_acc:.1%}\")\n",
    "print(f\"   LogReg Baseline:   {baseline_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launch Interactive UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio interface\n",
    "from src.qa_system import create_gradio_interface\n",
    "\n",
    "interface = create_gradio_interface(qa_system)\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Project Structure\n",
    "\n",
    "```\n",
    "VerdictVision/\n",
    "‚îú‚îÄ‚îÄ configs/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ config.py          # Configuration parameters\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_collection.py # Data download\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py   # Text extraction & chunking\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py      # Embedding creation\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ retrieval.py       # Hybrid search\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ llm.py            # LLM management\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ qa_system.py      # Main Q&A system\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py     # Evaluation metrics\n",
    "‚îú‚îÄ‚îÄ main.py               # CLI entry point\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## üéØ Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Dataset | 713 California Appellate Cases |\n",
    "| Chunks | ~3,500 text segments |\n",
    "| P@5 | 0.85+ |\n",
    "| Outcome Prediction Accuracy | 85%+ |\n",
    "| Avg Latency | < 3s per query |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
